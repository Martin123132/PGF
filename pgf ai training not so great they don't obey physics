import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import torchvision
import torchvision.transforms as transforms
import time

# ---------------------------------------
# Device
# ---------------------------------------
device = torch.device("cpu")

# ---------------------------------------
# PGF stabiliser (same as before)
# ---------------------------------------
Gamma = 0.15
alpha = 0.4
loss_hist = [0.0, 0.0]

def pgf_stabilise(L):
    L_prev, L_prev2 = loss_hist[-1], loss_hist[-2]
    nonlinear = -alpha * L * (L - L_prev)
    curvature = Gamma * (L - 2*L_prev + L_prev2)
    L_pgf = L + nonlinear + curvature
    loss_hist.append(L)
    return max(L_pgf, 1e-9)

# ---------------------------------------
# CIFAR-10 LOADING
# ---------------------------------------
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))
])

trainset = torchvision.datasets.CIFAR10(root="./data",train=True,
                                        download=True,transform=transform)
testset  = torchvision.datasets.CIFAR10(root="./data",train=False,
                                        download=True,transform=transform)

trainloader = torch.utils.data.DataLoader(trainset,batch_size=128,shuffle=True)
testloader  = torch.utils.data.DataLoader(testset, batch_size=256)

# ---------------------------------------
# SIMPLE CNN
# ---------------------------------------
class SmallCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(3,32,3,padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(32,64,3,padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
        )
        self.fc = nn.Sequential(
            nn.Linear(64*8*8,256),
            nn.ReLU(),
            nn.Linear(256,10)
        )
    def forward(self,x):
        x = self.conv(x)
        x = x.view(x.size(0), -1)
        return self.fc(x)

model_base = SmallCNN().to(device)
model_pgf  = SmallCNN().to(device)

opt_base = optim.Adam(model_base.parameters(), lr=0.001)
opt_pgf  = optim.Adam(model_pgf.parameters(),  lr=0.001)

loss_fn = nn.CrossEntropyLoss()

# ---------------------------------------
# TRAIN
# ---------------------------------------
epochs = 10
loss_base_curve = []
loss_pgf_curve  = []
tension_curve   = []

start = time.time()

for ep in range(epochs):
    print(f"\n=== Epoch {ep+1}/{epochs} ===")
    for i,(inp,lab) in enumerate(trainloader):
        inp,lab = inp.to(device), lab.to(device)

        # ==== baseline ====
        opt_base.zero_grad()
        out = model_base(inp)
        loss = loss_fn(out,lab)
        loss.backward()
        opt_base.step()
        loss_base_curve.append(loss.item())

        # ==== PGF version ====
        opt_pgf.zero_grad()
        out2 = model_pgf(inp)
        raw_loss = loss_fn(out2,lab)

        L_s = pgf_stabilise(raw_loss.item())
        scaled = raw_loss * (L_s / (raw_loss.item() + 1e-9))

        scaled.backward()
        opt_pgf.step()

        loss_pgf_curve.append(raw_loss.item())
        tension_curve.append(L_s)

        if i % 150 == 0:
            print(f"Batch {i:04d} | Base={loss.item():.4f} | PGF={raw_loss.item():.4f}")

print(f"\nTraining time (s): {time.time()-start:.1f}")

# ---------------------------------------
# TEST ACCURACY
# ---------------------------------------
def check_acc(model):
    correct = 0
    total = 0
    model.eval()
    with torch.no_grad():
        for inp,lab in testloader:
            inp,lab = inp.to(device), lab.to(device)
            out = model(inp)
            _,pred = out.max(1)
            correct += (pred==lab).sum().item()
            total += lab.size(0)
    return correct/total

acc_base = check_acc(model_base)
acc_pgf  = check_acc(model_pgf)

print("\nFinal accuracy baseline:", acc_base)
print("Final accuracy PGF:     ", acc_pgf)

# ---------------------------------------
# PLOTS
# ---------------------------------------
plt.figure(figsize=(12,4))
plt.plot(loss_base_curve, 'r--', alpha=0.6, label="Base")
plt.plot(loss_pgf_curve, 'g-', alpha=0.6, label="PGF")
plt.yscale("log")
plt.legend()
plt.title("Training Loss (10 epochs)")
plt.show()

plt.figure(figsize=(12,4))
plt.plot(tension_curve)
plt.title("PGF Loss Tension")
plt.xlabel("Step")
plt.ylabel("Tension")
plt.show()



100%|██████████| 170M/170M [00:02<00:00, 74.3MB/s]

=== Epoch 1/10 ===
Batch 0000 | Base=2.3049 | PGF=2.3036
Batch 0150 | Base=1.3365 | PGF=1.4317
Batch 0300 | Base=1.1117 | PGF=1.1741

=== Epoch 2/10 ===
Batch 0000 | Base=0.9797 | PGF=0.9867
Batch 0150 | Base=0.9083 | PGF=0.8998
Batch 0300 | Base=0.8864 | PGF=0.8917

=== Epoch 3/10 ===
Batch 0000 | Base=0.7401 | PGF=0.7251
Batch 0150 | Base=0.8933 | PGF=0.9154
Batch 0300 | Base=0.6796 | PGF=0.6894

=== Epoch 4/10 ===
Batch 0000 | Base=0.7453 | PGF=0.7265
Batch 0150 | Base=0.5711 | PGF=0.5747
Batch 0300 | Base=0.5304 | PGF=0.5224

=== Epoch 5/10 ===
Batch 0000 | Base=0.6760 | PGF=0.6608
Batch 0150 | Base=0.6016 | PGF=0.6227
Batch 0300 | Base=0.5834 | PGF=0.5837

=== Epoch 6/10 ===
Batch 0000 | Base=0.4646 | PGF=0.4896
Batch 0150 | Base=0.6932 | PGF=0.6089
Batch 0300 | Base=0.4386 | PGF=0.4453

=== Epoch 7/10 ===
Batch 0000 | Base=0.4195 | PGF=0.4330
Batch 0150 | Base=0.3968 | PGF=0.4069
Batch 0300 | Base=0.4254 | PGF=0.3886

=== Epoch 8/10 ===
Batch 0000 | Base=0.3459 | PGF=0.3233
Batch 0150 | Base=0.2008 | PGF=0.2315
Batch 0300 | Base=0.4383 | PGF=0.3750

=== Epoch 9/10 ===
Batch 0000 | Base=0.1706 | PGF=0.2001
Batch 0150 | Base=0.2917 | PGF=0.2179
Batch 0300 | Base=0.1604 | PGF=0.2120

=== Epoch 10/10 ===
Batch 0000 | Base=0.1822 | PGF=0.1998
Batch 0150 | Base=0.1373 | PGF=0.1064
Batch 0300 | Base=0.2661 | PGF=0.2137

Training time (s): 1579.2

Final accuracy baseline: 0.7266
Final accuracy PGF:      0.7259


import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as T
import matplotlib.pyplot as plt
import time

# ============================================================
# DATA
# ============================================================
transform = T.Compose([
    T.ToTensor(),
    T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
])

trainset = torchvision.datasets.CIFAR10(root="./cifar", train=True, download=True, transform=transform)
testset  = torchvision.datasets.CIFAR10(root="./cifar", train=False, download=True, transform=transform)

train_loader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)
test_loader  = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)

device = torch.device("cpu")
print("Using:", device)

# ============================================================
# MID-SIZED CNN
# ============================================================
class MidCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(),
            nn.Conv2d(32, 32, 3, padding=1), nn.ReLU(),
            nn.MaxPool2d(2),

            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),
            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(),
            nn.MaxPool2d(2),

            nn.Flatten(),
            nn.Linear(64*8*8, 256), nn.ReLU(),
            nn.Linear(256, 10)
        )

    def forward(self, x):
        return self.net(x)


model_base = MidCNN().to(device)
model_pgf  = MidCNN().to(device)

loss_fn = nn.CrossEntropyLoss()

opt_base = optim.Adam(model_base.parameters(), lr=0.001)
opt_pgf  = optim.Adam(model_pgf.parameters(),  lr=0.001)

# ============================================================
# PGF LOSS GEOMETRY
# ============================================================
Gamma = 0.12     # curvature diffusion
alpha = 0.35     # nonlinear correction
loss_hist = [1.0, 1.0]  # L(t-1), L(t-2)

def pgf_filter(L):
    L_prev, L_prev2 = loss_hist[-1], loss_hist[-2]
    nonlinear = -alpha * L * (L - L_prev)
    curvature = Gamma * (L - 2*L_prev + L_prev2)
    L_new = L + nonlinear + curvature
    loss_hist.append(L)
    return max(L_new, 1e-12)

# ============================================================
# TRAINING LOOP
# ============================================================
epochs = 10
base_losses = []
pgf_losses = []
pgf_tension = []
base_acc_list = []
pgf_acc_list = []

def test_accuracy(model):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for x, y in test_loader:
            x, y = x.to(device), y.to(device)
            preds = model(x)
            _, predicted = preds.max(1)
            total += y.size(0)
            correct += (predicted == y).sum().item()
    return correct / total


start = time.time()

for epoch in range(1, epochs + 1):
    print(f"\n=== Epoch {epoch}/{epochs} ===")
    model_base.train()
    model_pgf.train()

    for batch_idx, (x, y) in enumerate(train_loader):
        x, y = x.to(device), y.to(device)

        # ---------------- Baseline ----------------
        opt_base.zero_grad()
        out_b = model_base(x)
        Lb = loss_fn(out_b, y)
        Lb.backward()
        opt_base.step()
        base_losses.append(Lb.item())

        # ---------------- PGF ---------------------
        opt_pgf.zero_grad()
        out_p = model_pgf(x)
        Lp = loss_fn(out_p, y)

        Lp_scalar = Lp.item()
        Lp_filtered = pgf_filter(Lp_scalar)

        scaled_loss = Lp * (Lp_filtered / (Lp_scalar + 1e-9))
        scaled_loss.backward()
        opt_pgf.step()

        pgf_losses.append(Lp_scalar)
        pgf_tension.append(abs(Lp_filtered - Lp_scalar))

        if batch_idx % 150 == 0:
            print(f"Batch {batch_idx:04d} | Base={Lb.item():.4f} | PGF={Lp_scalar:.4f}")

    # accuracy per epoch
    base_acc = test_accuracy(model_base)
    pgf_acc  = test_accuracy(model_pgf)
    base_acc_list.append(base_acc)
    pgf_acc_list.append(pgf_acc)
    print(f"Acc (Base={base_acc:.4f}, PGF={pgf_acc:.4f})")

end = time.time()
print("\nTraining time (sec):", end - start)

print("\nFinal accuracy:")
print("Baseline:", base_acc_list[-1])
print("PGF:     ", pgf_acc_list[-1])

# ============================================================
# PLOTS
# ============================================================
plt.figure(figsize=(14,4))
plt.plot(base_losses, "r--", label="Baseline")
plt.plot(pgf_losses, "g-", label="PGF")
plt.yscale("log")
plt.title("Training Loss (log scale)")
plt.legend()
plt.show()

plt.figure(figsize=(14,4))
plt.plot(base_acc_list, "r--", label="Baseline")
plt.plot(pgf_acc_list, "g-", label="PGF")
plt.title("Test Accuracy per Epoch")
plt.legend()
plt.show()

plt.figure(figsize=(14,4))
plt.plot(pgf_tension)
plt.title("PGF Loss Tension Over Time")
plt.ylabel("Tension")
plt.xlabel("Step")
plt.show()


100%|██████████| 170M/170M [00:03<00:00, 54.8MB/s]
Using: cpu

=== Epoch 1/10 ===
Batch 0000 | Base=2.2973 | PGF=2.3038
Batch 0150 | Base=1.7312 | PGF=1.7447
Batch 0300 | Base=1.4529 | PGF=1.4656
Batch 0450 | Base=1.2912 | PGF=1.3078
Batch 0600 | Base=1.3817 | PGF=1.3845
Batch 0750 | Base=1.2699 | PGF=1.2423
Acc (Base=0.6113, PGF=0.6112)

=== Epoch 2/10 ===
Batch 0000 | Base=1.0147 | PGF=1.0665
Batch 0150 | Base=1.0266 | PGF=1.0949
Batch 0300 | Base=0.9426 | PGF=0.9113
Batch 0450 | Base=1.1100 | PGF=1.1036
Batch 0600 | Base=0.8952 | PGF=0.9588
Batch 0750 | Base=0.7182 | PGF=0.7629
Acc (Base=0.6778, PGF=0.6649)

=== Epoch 3/10 ===
Batch 0000 | Base=0.6745 | PGF=0.6765
Batch 0150 | Base=1.0102 | PGF=0.9422
Batch 0300 | Base=0.8684 | PGF=0.9659
Batch 0450 | Base=0.7551 | PGF=0.6736
Batch 0600 | Base=0.7277 | PGF=0.7438
Batch 0750 | Base=0.8999 | PGF=0.8390
Acc (Base=0.7384, PGF=0.7278)

=== Epoch 4/10 ===
Batch 0000 | Base=0.4870 | PGF=0.5882
Batch 0150 | Base=0.5703 | PGF=0.5678
Batch 0300 | Base=0.4277 | PGF=0.4427
Batch 0450 | Base=0.7830 | PGF=0.8604
Batch 0600 | Base=0.5006 | PGF=0.5532
Batch 0750 | Base=0.6412 | PGF=0.5825
Acc (Base=0.7477, PGF=0.7406)

=== Epoch 5/10 ===
Batch 0000 | Base=0.3480 | PGF=0.4057
Batch 0150 | Base=0.3556 | PGF=0.4040
Batch 0300 | Base=0.4730 | PGF=0.5242
Batch 0450 | Base=0.3183 | PGF=0.4059
Batch 0600 | Base=0.5304 | PGF=0.3866
Batch 0750 | Base=0.5992 | PGF=0.6148
Acc (Base=0.7442, PGF=0.7543)

=== Epoch 6/10 ===
Batch 0000 | Base=0.3497 | PGF=0.1849
Batch 0150 | Base=0.2346 | PGF=0.2520
Batch 0300 | Base=0.2489 | PGF=0.2881
Batch 0450 | Base=0.3300 | PGF=0.2663
Batch 0600 | Base=0.2874 | PGF=0.3973
Batch 0750 | Base=0.2465 | PGF=0.4136
Acc (Base=0.7477, PGF=0.7534)

=== Epoch 7/10 ===
Batch 0000 | Base=0.1607 | PGF=0.1824
Batch 0150 | Base=0.2453 | PGF=0.2543
Batch 0300 | Base=0.1475 | PGF=0.2266
Batch 0450 | Base=0.2656 | PGF=0.2493
Batch 0600 | Base=0.2162 | PGF=0.2048
Batch 0750 | Base=0.2675 | PGF=0.1169
Acc (Base=0.7430, PGF=0.7397)

=== Epoch 8/10 ===
Batch 0000 | Base=0.1579 | PGF=0.1030
Batch 0150 | Base=0.2506 | PGF=0.1169
Batch 0300 | Base=0.0330 | PGF=0.0380
Batch 0450 | Base=0.0483 | PGF=0.1968
Batch 0600 | Base=0.2662 | PGF=0.1775
Batch 0750 | Base=0.1114 | PGF=0.1983
Acc (Base=0.7464, PGF=0.7502)

=== Epoch 9/10 ===
Batch 0000 | Base=0.0638 | PGF=0.1131
Batch 0150 | Base=0.1183 | PGF=0.1542
Batch 0300 | Base=0.0913 | PGF=0.1477
Batch 0450 | Base=0.1259 | PGF=0.0584
Batch 0600 | Base=0.0925 | PGF=0.1470
Batch 0750 | Base=0.1209 | PGF=0.1595
Acc (Base=0.7431, PGF=0.7375)

=== Epoch 10/10 ===
Batch 0000 | Base=0.0392 | PGF=0.0753
Batch 0150 | Base=0.0759 | PGF=0.1153
Batch 0300 | Base=0.0811 | PGF=0.1033
Batch 0450 | Base=0.0706 | PGF=0.1299
Batch 0600 | Base=0.1819 | PGF=0.2157
Batch 0750 | Base=0.0821 | PGF=0.0525
Acc (Base=0.7371, PGF=0.7411)

Training time (sec): 4078.960363149643

Final accuracy:
Baseline: 0.7371
PGF:      0.7411

!pip install datasets --quiet

import torch
import torch.nn as nn
import torch.optim as optim
from datasets import load_dataset
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt

# ================================================================
# LOAD AG NEWS (HuggingFace version)
# ================================================================
dataset = load_dataset("ag_news")

train_texts = dataset["train"]["text"]
train_labels = dataset["train"]["label"]

test_texts = dataset["test"]["text"]
test_labels = dataset["test"]["label"]

# ================================================================
# TOKENISER (simple, no dependencies)
# ================================================================
import re
def tokenize(t):
    return re.findall(r"\b\w+\b", t.lower())

# Build vocab
from collections import Counter
counter = Counter()
for t in train_texts:
    counter.update(tokenize(t))

vocab = {w: i+2 for i, (w, _) in enumerate(counter.most_common(30000))}
vocab["<unk>"] = 0
vocab["<pad>"] = 1

def encode(text):
    return torch.tensor([vocab.get(w, 0) for w in tokenize(text)], dtype=torch.long)

# ================================================================
# DATASET WRAPPERS
# ================================================================
def collate(batch):
    texts, labels = zip(*batch)
    enc = [encode(t) for t in texts]
    padded = pad_sequence(enc, batch_first=True, padding_value=1)
    return padded, torch.tensor(labels)

train_ds = list(zip(train_texts, train_labels))
test_ds  = list(zip(test_texts,  test_labels))

train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate)
test_loader  = DataLoader(test_ds,  batch_size=64, shuffle=False, collate_fn=collate)

# ================================================================
# SIMPLE MODEL (same as before)
# ================================================================
class TextClassifier(nn.Module):
    def __init__(self, vocab_size, embed_dim=64, hidden=64, num_classes=4):
        super().__init__()
        self.emb = nn.Embedding(vocab_size, embed_dim, padding_idx=1)
        self.rnn = nn.GRU(embed_dim, hidden, batch_first=True)
        self.fc = nn.Linear(hidden, num_classes)

    def forward(self, x):
        e = self.emb(x)
        _, h = self.rnn(e)
        return self.fc(h[-1])

vocab_size = len(vocab)
model_base = TextClassifier(vocab_size)
model_pgf  = TextClassifier(vocab_size)

loss_fn = nn.CrossEntropyLoss()
opt_base = optim.Adam(model_base.parameters(), lr=0.001)
opt_pgf  = optim.Adam(model_pgf .parameters(), lr=0.001)

# ================================================================
# PGF LOSS MODULE
# ================================================================
Gamma = 0.12
alpha = 0.35
history = [1.0, 1.0]

def pgf_modify(L):
    L1, L2 = history[-1], history[-2]
    nonlinear = -alpha * L * (L - L1)
    curvature = Gamma * (L - 2*L1 + L2)
    L_new = max(L + nonlinear + curvature, 1e-9)
    history.append(L)
    return L_new

# ================================================================
# TRAINING
# ================================================================
def evaluate(model):
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for x, y in test_loader:
            pred = model(x)
            correct += (pred.argmax(dim=1) == y).sum().item()
            total += len(y)
    return correct / total

epochs = 3
loss_base, loss_pgf = [], []
acc_base, acc_pgf = [], []

for ep in range(epochs):
    model_base.train()
    model_pgf.train()

    for batch_idx, (x, y) in enumerate(train_loader):
        # ---- BASELINE ----
        opt_base.zero_grad()
        out = model_base(x)
        loss = loss_fn(out, y)
        loss.backward()
        opt_base.step()
        loss_base.append(loss.item())

        # ---- PGF ----
        opt_pgf.zero_grad()
        out2 = model_pgf(x)
        loss2 = loss_fn(out2, y)
        Ls = pgf_modify(loss2.item())
        scaled = loss2 * (Ls / (loss2.item() + 1e-12))
        scaled.backward()
        opt_pgf.step()
        loss_pgf.append(loss2.item())

        if batch_idx % 300 == 0:
            print(f"Epoch {ep} | Batch {batch_idx} | Base={loss.item():.4f} | PGF={loss2.item():.4f}")

    # ---- End of epoch accuracy ----
    acc_base.append(evaluate(model_base))
    acc_pgf.append(evaluate(model_pgf))
    print(f"ACC base={acc_base[-1]:.4f} | pgf={acc_pgf[-1]:.4f}")

# ================================================================
# PLOTS
# ================================================================
plt.figure(figsize=(14,6))
plt.plot(loss_base, 'r--', label="Base")
plt.plot(loss_pgf, 'g-', label="PGF")
plt.yscale("log")
plt.legend()
plt.title("Training Loss (log scale)")
plt.show()

plt.figure(figsize=(10,4))
plt.plot(acc_base, 'r--', label="Base acc")
plt.plot(acc_pgf, 'g-', label="PGF acc")
plt.title("Test Accuracy per Epoch")
plt.legend()
plt.show()


/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: 
The secret `HF_TOKEN` does not exist in your Colab secrets.
To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.
You will be able to reuse this secret in all of your notebooks.
Please note that authentication is recommended but still optional to access public models or datasets.
  warnings.warn(
README.md: 
 8.07k/? [00:00<00:00, 219kB/s]
data/train-00000-of-00001.parquet: 100%
 18.6M/18.6M [00:01<00:00, 22.1MB/s]
data/test-00000-of-00001.parquet: 100%
 1.23M/1.23M [00:00<00:00, 2.40MB/s]
Generating train split: 100%
 120000/120000 [00:00<00:00, 219152.39 examples/s]
Generating test split: 100%
 7600/7600 [00:00<00:00, 179067.55 examples/s]
Epoch 0 | Batch 0 | Base=1.3853 | PGF=1.3964
Epoch 0 | Batch 300 | Base=1.3863 | PGF=1.3861
Epoch 0 | Batch 600 | Base=1.2806 | PGF=1.3174
Epoch 0 | Batch 900 | Base=1.3083 | PGF=1.1611
Epoch 0 | Batch 1200 | Base=1.1968 | PGF=0.8530
Epoch 0 | Batch 1500 | Base=1.1215 | PGF=0.8450
Epoch 0 | Batch 1800 | Base=0.9984 | PGF=0.4362
Epoch 0 | Batch 2100 | Base=1.0138 | PGF=0.4582
Epoch 0 | Batch 2400 | Base=1.1140 | PGF=0.3823
Epoch 0 | Batch 2700 | Base=0.8437 | PGF=0.3162
Epoch 0 | Batch 3000 | Base=1.0197 | PGF=0.2751
Epoch 0 | Batch 3300 | Base=0.9165 | PGF=0.3246
Epoch 0 | Batch 3600 | Base=0.8550 | PGF=0.2828
ACC base=0.6191 | pgf=0.8832
Epoch 1 | Batch 0 | Base=0.9343 | PGF=0.3631
Epoch 1 | Batch 300 | Base=0.8193 | PGF=0.2619
Epoch 1 | Batch 600 | Base=0.6560 | PGF=0.1807
Epoch 1 | Batch 900 | Base=0.6539 | PGF=0.1139
Epoch 1 | Batch 1200 | Base=0.4829 | PGF=0.2644
Epoch 1 | Batch 1500 | Base=0.5411 | PGF=0.1699
Epoch 1 | Batch 1800 | Base=0.2322 | PGF=0.0947
Epoch 1 | Batch 2100 | Base=0.6908 | PGF=0.5661
Epoch 1 | Batch 2400 | Base=0.6048 | PGF=0.3031
Epoch 1 | Batch 2700 | Base=0.5387 | PGF=0.4377
Epoch 1 | Batch 3000 | Base=0.4333 | PGF=0.4176
Epoch 1 | Batch 3300 | Base=0.5243 | PGF=0.4281
Epoch 1 | Batch 3600 | Base=0.2872 | PGF=0.2573
ACC base=0.8772 | pgf=0.9067
Epoch 2 | Batch 0 | Base=0.4060 | PGF=0.2849
Epoch 2 | Batch 300 | Base=0.2865 | PGF=0.1368
Epoch 2 | Batch 600 | Base=0.4151 | PGF=0.1280
Epoch 2 | Batch 900 | Base=0.5179 | PGF=0.4967
Epoch 2 | Batch 1200 | Base=0.5611 | PGF=0.4683
Epoch 2 | Batch 1500 | Base=0.1974 | PGF=0.1149
Epoch 2 | Batch 1800 | Base=0.2785 | PGF=0.1601
Epoch 2 | Batch 2100 | Base=0.3421 | PGF=0.2575
Epoch 2 | Batch 2400 | Base=0.1516 | PGF=0.0553
Epoch 2 | Batch 2700 | Base=0.1950 | PGF=0.1763
Epoch 2 | Batch 3000 | Base=0.1245 | PGF=0.0659
Epoch 2 | Batch 3300 | Base=0.4757 | PGF=0.4043
Epoch 2 | Batch 3600 | Base=0.2370 | PGF=0.1776
ACC base=0.9050 | pgf=0.9117



import torch, torch.nn as nn, torch.optim as optim
import torchvision
import torchvision.transforms as T
import matplotlib.pyplot as plt
import numpy as np
from torch.utils.data import DataLoader

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

# ============================================================
# PGF Stabiliser
# ============================================================
class PGF:
    def __init__(self, Gamma=0.12, alpha=0.45):
        self.Gamma = Gamma
        self.alpha = alpha
        self.L_prev = 0.0
        self.L_prev2 = 0.0

    def __call__(self, L):
        nonlinear = -self.alpha * L * (L - self.L_prev)
        curvature = self.Gamma * (L - 2*self.L_prev + self.L_prev2)
        L_adj = max(L + nonlinear + curvature, 1e-9)
        self.L_prev2 = self.L_prev
        self.L_prev = L
        return L_adj


# ============================================================
# CIFAR-10 DATA
# ============================================================
transform = T.Compose([
    T.RandomHorizontalFlip(),
    T.RandomCrop(32, padding=4),
    T.ToTensor(),
])

trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True, transform=transform)
testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                       download=True, transform=T.ToTensor())

trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)
testloader = DataLoader(testset, batch_size=256, shuffle=False, num_workers=2)


# ============================================================
# MODELS — ResNet-18
# ============================================================
resnet_base = torchvision.models.resnet18(weights=None, num_classes=10).to(device)
resnet_pgf  = torchvision.models.resnet18(weights=None, num_classes=10).to(device)

criterion = nn.CrossEntropyLoss()
opt_base = optim.Adam(resnet_base.parameters(), lr=0.001)
opt_pgf  = optim.Adam(resnet_pgf.parameters(),  lr=0.001)

pgf = PGF(Gamma=0.12, alpha=0.45)

# ============================================================
# TRAIN + TEST FUNCTIONS
# ============================================================
def test(model):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for x, y in testloader:
            x, y = x.to(device), y.to(device)
            out = model(x)
            _, pred = out.max(1)
            total += y.size(0)
            correct += (pred == y).sum().item()
    return correct / total


# ============================================================
# TRAIN LOOP
# ============================================================
epochs = 5
loss_base_hist = []
loss_pgf_hist  = []
acc_base_hist  = []
acc_pgf_hist   = []

for epoch in range(epochs):
    print(f"\n=== Epoch {epoch+1}/{epochs} ===")

    resnet_base.train()
    resnet_pgf.train()

    for i, (x, y) in enumerate(trainloader):
        x, y = x.to(device), y.to(device)

        # ---- BASE ----
        opt_base.zero_grad()
        out = resnet_base(x)
        L = criterion(out, y)
        L.backward()
        opt_base.step()
        loss_base_hist.append(L.item())

        # ---- PGF ----
        opt_pgf.zero_grad()
        out2 = resnet_pgf(x)
        L2 = criterion(out2, y)
        L_adj = pgf(L2.item())
        scaled_loss = L2 * (L_adj / (L2.item() + 1e-9))
        scaled_loss.backward()
        opt_pgf.step()
        loss_pgf_hist.append(L2.item())

        if i % 100 == 0:
            print(f"Batch {i} | Base={L.item():.4f} | PGF={L2.item():.4f}")

    acc_base = test(resnet_base)
    acc_pgf  = test(resnet_pgf)
    acc_base_hist.append(acc_base)
    acc_pgf_hist.append(acc_pgf)

    print(f"ACC base={acc_base:.4f} | pgf={acc_pgf:.4f}")

# ============================================================
# PLOTS
# ============================================================
plt.figure(figsize=(12,5))
plt.title("Training Loss (log scale)")
plt.plot(loss_base_hist, 'r--', label="Base")
plt.plot(loss_pgf_hist, 'g-',  label="PGF")
plt.yscale("log")
plt.legend()
plt.show()

plt.figure(figsize=(10,4))
plt.title("Test Accuracy per Epoch")
plt.plot(acc_base_hist, 'r--', label="Base acc")
plt.plot(acc_pgf_hist,  'g-', label="PGF acc")
plt.legend()
plt.show()



Device: cpu

=== Epoch 1/5 ===
Batch 0 | Base=2.6819 | PGF=2.6149
Batch 100 | Base=1.7875 | PGF=1.8439
Batch 200 | Base=1.3308 | PGF=1.2535
Batch 300 | Base=1.3460 | PGF=1.2648
ACC base=0.4972 | pgf=0.4725

=== Epoch 2/5 ===
Batch 0 | Base=1.3581 | PGF=1.3169
Batch 100 | Base=1.0988 | PGF=1.1896
Batch 200 | Base=0.9852 | PGF=1.0176
Batch 300 | Base=1.2332 | PGF=1.2281
ACC base=0.5759 | pgf=0.6013

=== Epoch 3/5 ===
Batch 0 | Base=0.9096 | PGF=0.9557
Batch 100 | Base=1.0781 | PGF=1.0688
Batch 200 | Base=1.0492 | PGF=1.0242
Batch 300 | Base=1.0156 | PGF=1.0728
ACC base=0.6563 | pgf=0.5895

=== Epoch 4/5 ===
Batch 0 | Base=0.9398 | PGF=0.9028
Batch 100 | Base=0.8829 | PGF=1.0147
Batch 200 | Base=0.7960 | PGF=0.8022
Batch 300 | Base=0.7569 | PGF=0.8198
ACC base=0.6424 | pgf=0.6410

=== Epoch 5/5 ===
Batch 0 | Base=0.8316 | PGF=0.8197
Batch 100 | Base=0.8011 | PGF=0.8549
Batch 200 | Base=0.8967 | PGF=0.9186
Batch 300 | Base=0.9322 | PGF=0.9240
ACC base=0.6277 | pgf=0.6714














